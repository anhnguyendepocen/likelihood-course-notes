\documentclass[11pt]{article}
\usepackage{graphicx}
\DeclareGraphicsRule{.tif}{png}{.png}{`convert #1 `dirname #1`/`basename #1 .tif`.png}

\textwidth = 6.5 in
\textheight = 9 in
\oddsidemargin = 0.0 in
\evensidemargin = 0.0 in
\topmargin = 0.0 in
\headheight = 0.0 in
\headsep = 0.0 in
\parskip = 0.2in
\parindent = 0.0in
\usepackage{paralist} %compactenum

%\newtheorem{theorem}{Theorem}
%\newtheorem{corollary}[theorem]{Corollary}
%\newtheorem{definition}{Definition}
\usepackage{tipa}
\usepackage{amsfonts}
\usepackage[mathscr]{eucal}

% Use the natbib package for the bibliography
\usepackage[round]{natbib}
\bibliographystyle{apalike}
\newcommand{\prop}[2]{q(#1,#2)}
\newcommand{\accept}[2]{\alpha({#1,#2})}

\renewcommand{\Pr}{{\mathbb P}}
\usepackage{wrapfig}
\usepackage{bm}
\usepackage{listings}
\usepackage{url}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{pgf}
\include{positioning}
\usepackage{tikz}
\usetikzlibrary{trees,arrows,positioning,scopes}
\tikzset{terminal/.style={rectangle,minimum size=6mm,rounded corners=3mm,very thick,draw=black!50, top color=white,bottom color=black!20, font=\ttfamily}}
\tikzset{hidden/.style={rectangle,draw=white,fill=white,thick}}
\tikzset{analysis/.style={rectangle,rounded corners,draw=black!50,fill=white,thick,minimum width=6cm}}
\tikzset{charmatrix/.style={rectangle,draw=none,fill=black,minimum width=6cm,minimum height=8mm}}
\tikzset{augmat/.style={rectangle,draw=none,fill=red,minimum width=6cm,minimum height=15mm}}
\tikzset{tree/.style={rectangle,draw=black,fill=black,minimum width=6cm,minimum height=8mm}}
\tikzset{inf/.style={rectangle,rounded corners,draw=black!50,fill=green!20,thick,minimum width=6cm,minimum height=2cm}}
\tikzset{toArrow/.style={stealth-,ultra thick}}


%\newcommand{\newAppendix}[2]{
%	\addtocounter{appendixCounter}{1}
%	{\bf Appendix {\arabic{appendixCounter}\label{#2}}: #1}
%	}


\usepackage{hyperref}
\hypersetup{backref,  linkcolor=blue, citecolor=red, colorlinks=true, hyperindex=true}

\begin{document}
\newcounter{appendixCounter}

\section*{MCMC notes by Mark Holder}
\subsection*{Bayesian inference}
Ultimately, we want to make probability statements about true values of parameters, given our data. For example $\Pr(\alpha_0 < \alpha_1|X)$.
According to Bayes' theorem:
	$$\Pr(\theta|X) = \frac{\Pr(X|\theta)\Pr(\theta)}{\Pr(X)}$$
It is often the case that we cannot calculate $\Pr(X)$, the marginal probability of the data.


Markov chain Monte Carlo is (by far), the dominant computational approach to conducting Bayesian inference.
The ``Monte Carlo'' part of the name reflects the fact that we are going to perform a random simulation -- the algorithm uses a pseudo-random number generator to explore parameter space.
The ``Markov chain'' part of the name refers to fact that we are going to conduct the simulation by iteratively updating the state of the simulator using rules that depend only on the current state of the system.
This type of stochastic system (one which ``forgets'' its history) is referred to as a Markov chain.

\subsection*{Markov chains}
A Markov chain connects a series of states.
You can think of it as a walk through a ``state space''.
\begin{compactitem}
	\item a {\em state} is the complete description of the ``value'' of the chain as it walks.  We'll be walking through parameter space, so the state space is the space of all parameter values in our model.
	Typically the state space is continuous, but I'll explain most of the theory in problems with discrete state space.
	\item an {\em index} refers to which step of the chain we are currently at. There are many uses for continuous-time Markov chains (in which the index can assume any numerical value), but the Markov chains that we will be using in MCMC are discrete time.  This means that the index is simply a count of the number iterations that the algorithm has been running.  The index is often referred to as the ``iteration,'' the ``generation,'' or the ``step number.''  In a discrete time Markov chain, you update the state once per interval (although you can remain in the same state and still consider that an update).
\end{compactitem}

There is a huge literature on Markov processes, but there are only a few crucial aspects of Markov chains that we need to understand if we want to understand MCMC.
\begin{compactitem}
	\item Under mild conditions, a Markov chain will converge to a stationary distribution (also called a steady-state distribution) after a large number of steps. This distribution will be the same regardless of the starting point of the chain.
	\item Markov chains that have a high probability of changing state in a single step will converge to the stationary distribution faster.
\end{compactitem}

\subsubsection*{Transition probabilities}
A Markov chain can be defined by describing the full set of probability statements that define the rules for the state of the chain in the next step (step $i+1$) given its current state (in step $i$).
These transition probabilities are analogous to transition rates if we are working with continuous-time Markov processes.

\begin{wrapfigure}{r}{5cm}
\begin{tikzpicture} 
  \node[terminal] (zero) {$0$};
  \node[terminal,right=2cm of zero] (one) {$1$};
  \draw[->] (zero) edge[bend left] node (f) [above]{0.6} (one)
  		    (zero) edge [out=250, in=110, looseness=1,distance=3cm, loop] node[left] {$0.4$} (zero)
            (one) edge[bend left] node (h) [below]{$0.9$} (zero)
            (one) edge [out=290, in=70, looseness=1,distance=2cm, loop] node[right] {$0.1$} (zero)
            ;
\end{tikzpicture}
\caption{A graphical depiction of a two-state Markov process.}\label{fig2state}
\end{wrapfigure}
Consider the simplest possible Markov chain: one with two states (0, and 1) that operates in discrete time.
The figure to the right shows the states in circles. 
The transition probabilities are shown as arcs connecting the states with the probabilities next to the line.
The full probability statements that correspond to the graph are:
\begin{eqnarray*}
	\Pr(x_{i+1}=0|x_i=0) & = & 0.4\\
	\Pr(x_{i+1}=1|x_i=0) & = & 0.6\\
	\Pr(x_{i+1}=0|x_i=1) & = & 0.9\\
	\Pr(x_{i+1}=1|x_i=1) & = & 0.1
\end{eqnarray*}
Note that, because these are probabilities, some of them must sum to one.
In particular, if we are in a particular state at step $i$ we can call the state $x_i$.  
In the next step, we must have {\em some} state so $1 = \sum_j \Pr(x_{i+1}=j|x_i)$ for every possible $x_i$.

Note that the state at step $i+1$ only depends on the state at step $i$. 
This is the Markov property. More formally we could state it as:
$$\Pr(x_{i+1}|x_i) = \Pr(x_{i+1}|x_i, x_{i-k}) $$
where $k$ is positive integer. 
What this probability statement is saying is that, conditional on $x_i$, the state at $i+1$ is independent on the state at any point before $i$.
So when working with Markov chains we don't need to concern ourselves with the full history of the chain, merely knowing the state at the previous step is enough.\footnote{There are second-order Markov processes that depend on the two previous states, and third-order Markov process {\em etc.}. But in this course, we'll just be dealing with the simplest Markov chains which only depend on the current state.}

Clearly if we know $x_i$ and the transition probabilities, then we can make a probabilistic statement about the state in the next iteration (in fact the transition probabilities {\em are} these probabilistic statements).
But we can also think about the probability that the chain will be in a particular state two steps form now:
\begin{eqnarray*}
	\Pr(x_{i+2}=0|x_i=0) & = & \Pr(x_{i+2}=0|x_{i+1}=1)\Pr(x_{i+1}=1|x_i=0) +  \Pr(x_{i+2}=0|x_{i+1}=0)\Pr(x_{i+1}=0|x_i=0)\\
					    & = & 0.9*0.6 +  0.4*0.4\\
					    & = & 0.7
\end{eqnarray*}
Here we are exploiting the fact that the same ``rules'' (transition probabilities) apply when we consider state changes between $i+1$ and $i+2$.
If the transition probabilities are fixed through the running of the Markov chain, then we are dealing with a time-homogeneous Markov chain.

Note that:
\begin{eqnarray*}
	\Pr(x_{i+2}=1|x_i=0) & = & \Pr(x_{i+2}=1|x_{i+1}=1)\Pr(x_{i+1}=1|x_i=0) +  \Pr(x_{i+2}=1|x_{i+1}=0)\Pr(x_{i+1}=0|x_i=0)\\
					    & = & 0.1*0.6 +  0.6*0.4\\
					    & = & 0.3
\end{eqnarray*}
So,  if we sum over all possible states at $i+2$, the relevant probability statements sum to one.

It gets tedious to continue this for a large number of steps into the future.
But we can ask a computer to calculate the probability of being in state $0$ for a large number of steps into the future by repeating the calculations (see the Appendix \ref{appendixCodeTwoProb} for code).
Figure (\ref{figProbTrace}) shows the probabilities of the Markov process being in state 0 as a function of the step number for the two possible starting states.

\begin{figure}[h]
\begin{picture}(0,230)(0,-100)
	\put(-30,20){\makebox(0,0)[l]{\includegraphics[scale=0.5]{start_from_0.pdf}}}
	\put(220,20){\makebox(0,0)[l]{\includegraphics[scale=0.5]{start_from_1.pdf}}}
	\put(50,-105){\small $\Pr(x_i=0|x_0=0)$}
	\put(310,-105){\small $\Pr(x_i=0|x_0=1)$}
\end{picture}
\caption{The probability of being in state 0 as a function of the step number, $i$, for two different starting states (0 and 1) for the Markov process depicted in Figure (\ref{fig2state}).}\label{figProbTrace}
\end{figure}

Note that the probability stabilizes to a steady state distribution.
Knowing whether the chain started in state 0 or 1 tells you very little about the state of the chain in step 15.
Technically, $x_{15}$ and $x_0$ are {\em not} independent of each other.
If you work through the math the probability of the state at step 15 does depend on the starting state:
\begin{eqnarray*}
	\Pr(x_{15} = 0 | x_0 = 0) & = & 0.599987792969\\
	\Pr(x_{15} = 0 | x_0 = 1) & = & 0.600018310547
\end{eqnarray*}
But clearly these two probabilities are very close to being equal.

If we consider an even larger number of iterations (e.g. the state at step 100), then the probabilities are so close that they are indistinguishable.

After a long enough walk, a Markov chain in which all of the states are connected\footnote{The fancy jargon is ``irreducible.'' A reducible chain has some states that are disconnected from others, so the chain can be broken up (reduced) into two or more chains that operate on a subset of the states.} will {\em converge} to its stationary distribution.

\subsubsection*{Many chains or one really long chain}
When constructing arguments about Markov chains we often flip back and forth between thinking about the behavior of an arbitrarily large number of instances of the random process all obeying the same rules (but performing their walks independently) versus the behavior we expect if we sparsely sample a very long chain.
The idea is that if sample sparsely enough, the sampled points from one chain are close to being independent of each other (as Figure \ref{figProbTrace} was meant to demonstrate).
Thus we can think of a very long chain as if we had a large number of independent chains.


\subsubsection*{Stationary distribution}
Notice in the discussion above that the probability of ending up in state 0 as the number of iterations increased approached 0.6. What is special about this value?

It turns out that in the stationary distribution (frequently denoted $\bm \pi$), the probability of sampling a chain in state 0 is 0.6.
We would write this as $\bm \pi = \{0.6, 0.4\}$  or the pair of statements: $\bm \pi_0 = 0.6, \bm \pi_1 = 0.4$.

How can we show that this is the steady-state (equilibrium) distribution?
The flux ``out'' of each state must be equal to the ``flux'' into that state for a system to be at equilibrium.
Here it is helpful to think about running infinitely many chains.
If we have an equilibrium probability distribution, then we can characterize the probability of a chain leaving state 0 in at one particular step in the process.
This is the joint probability of being in state 0 at step $i$ and the probability of an $0 \rightarrow 1 $ transition at point $i$.
Thus,
\begin{eqnarray*}
	\Pr(0 \rightarrow 1 \mbox{ at } i) & = & \Pr(x_i = 0)\Pr(x_{i+1} = 1| x_i = 0)
\end{eqnarray*}
At the equilibrium $ \Pr(x_i = 0) = \bm \pi_0$, so the flux out of 0 at step $i$ is $\bm \pi_0 \Pr(x_{i+1} = 1| x_i = 0)$.
In our simple (two-state) system the flux into state 0 can only come from state 1, so:
\begin{eqnarray*}
	\Pr(1 \rightarrow 0 \mbox{ at } i) & = & \Pr(x_i = 1)\Pr(x_{i+1} = 0| x_i = 1) \\
		& = & \bm \pi_1\Pr(x_{i+1} = 0| x_i = 1) 
\end{eqnarray*}
If we force the ``flux out'' and ``flux in'' to be identical:
\begin{eqnarray*}
	\bm \pi_0 \Pr(x_{i+1} = 1| x_i = 0) & = & \bm \pi_1\Pr(x_{i+1} = 0| x_i = 1) \\
	\frac{\bm \pi_0}{\bm \pi_1} & = & \frac{\Pr(x_{i+1} = 0| x_i = 1)}{\Pr(x_{i+1} = 1| x_i = 0)} 
\end{eqnarray*}
We can then solve for the actual frequencies by recognizing that $\bm \pi$ is a probability distribution:
\begin{eqnarray*}
	\sum_k \bm \pi_k & = & 1
\end{eqnarray*}
For the system that we are considering:
\begin{eqnarray*}
	\frac{\bm \pi_0}{\bm \pi_1} & = & \frac{0.9}{0.6} = 1.5  \\
	\bm \pi_0 & = & 1.5\bm \pi_1 \\
	1.5\bm \pi_1 + \bm \pi_1 & = & 1.0 \\
	\bm \pi_1 & = & 0.4 \\
	\bm \pi_0 & = &  0.6
\end{eqnarray*}

Thus, the transition probabilities determine the stationary distribution.

{\bf If we can choose the transition probabilities then we can construct a sampler that will converge to any distribution that we would like.}

When we have a state set $\mathcal{S}$ with more than 2 states, we can express a general statement about the stationary distribution:
\begin{equation}
	\bm\pi_j = \sum_{k\in \mathcal{S}}\bm\pi_{k}\Pr(x_{i+1} = j|x_{i} = k) \label{generalEquil}
\end{equation}
The connection to the ``flux out'' and ``flux in'' argument may not seem obvious, but note that the ``flux out'' can be 
quantified in terms of the 1 - the ``self-transition.''
Let $\mathcal{S}_{\neq j}$ be the set of states that excludes state $j$, so the ``flux out'' is:
\begin{eqnarray*}
	\mbox{flux out of } j & = & \bm\pi_j\Pr(x_{i+1}\in\mathcal{S}_{\neq j}|x_i=j) \\
	 & = &  \bm\pi_j\left[1 - \Pr(x_{i+1}\in j|x_i=j)\right]\\
\end{eqnarray*}
The flux in is:
\begin{eqnarray*}
	\mbox{flux into } j & = & \sum_{k\in \mathcal{S}_{\neq j}}\pi_{k}\Pr(x_{i+1} = j|x_{i} = k) \\
\end{eqnarray*}
Forcing the fluxes to be equal gives us:
\begin{eqnarray}
	\bm\pi_j\left[1 - \Pr(x_{i+1} =  j|x_i=j)\right] & = & \sum_{k\in \mathcal{S}_{\neq j}}\bm\pi_{k}\Pr(x_{i+1} = j|x_{i} = k) \nonumber \\
	\bm\pi_j & = &  \bm\pi_j\Pr(x_{i+1} =  j|x_i=j) + \sum_{k\in\mathcal{S}_{\neq j}}\bm \pi_{k}\Pr(x_{i+1} = j|x_{i} = k)    \label{separatedGenEq} \\
	& = & \sum_{k\in\mathcal{S}}\bm \pi_{k}\Pr(x_{i+1} = j|x_{i} = k) \nonumber
\end{eqnarray}



\subsubsection*{Mixing}
We just saw how changing transition probabilities will affect the stationary distribution. 
Perhaps, there is a one-to-one mapping between transition probabilities and a stationary distribution.
In the form of a question: if we were given the stationary distribution could we decide what the transition rates must be to achieve that distribution?

It turns out that the answer is ``No.''  
Which can be seen if we examine equation (\ref{separatedGenEq}).
Imagine dropping the ``flux out'' of each state by a factor of 10.
Because the ``self-transition'' rate would increase by the appropriate amount, we can still end up in the same stationary distribution.

How would such a chain behave? The primary difference is that it would ``mix'' more slowly.
Adjacent steps would be more likely to be in the same state, and it would take a larger number of iterations before the chain ``forgets'' its starting state.
Figure (\ref{figProbTraceSlow}) depicts the same probability statements as Figure (\ref{figProbTrace}) but for a process in which $\Pr(x_{i+1}=1|x_i=0) = 0.06 $ and $\Pr(x_{i+1}=0|x_i=1) = 0.09$.
\begin{figure}[h]
\begin{picture}(0,230)(0,-100)
	\put(-30,20){\makebox(0,0)[l]{\includegraphics[scale=0.5]{start_from_0_slow.pdf}}}
	\put(220,20){\makebox(0,0)[l]{\includegraphics[scale=0.5]{start_from_1_slow.pdf}}}
	\put(50,-105){\small $\Pr(x_i=0|x_0=0)$}
	\put(310,-105){\small $\Pr(x_i=0|x_0=1)$}
\end{picture}
\caption{The probability of being in state 0 as a function of the step number, $i$, for two different starting states (0 and 1) for the Markov process depicted in Figure (\ref{fig2state}) but with the state-changing transition probabilities scaled down by a factor of 10.  Compare this to Figure (\ref{figProbTrace}).}\label{figProbTraceSlow}
\end{figure}

Thus, that the rate of convergence of a chain to its stationary distribution is an aspect of a Markov chain that is separate from what the stationary distribution is.

In MCMC we will design a Markov chain such that its stationary distribution will be identical to the posterior probability distribution over the space of parameters.
We will try to design chains that have high transition probabilities, so that our MCMC approximation will quickly converge to the posterior.
But even a slowly mixing chain will (in theory) eventually be capable of providing an approximation to the posterior probability.


\subsection*{Detailed Balance}
In principle, we could use the information in Equation (\ref{generalEquil}) to inform us as to how to construct a chain with a desired $\bm \pi$.
In practice, this is difficult when we have a large number of states.
In the continuous parameter case, we have an infinite number of states and the summation becomes an integral.
Setting transition probabilities in a general way such that the total flux into and out of any state satisfies the property shown in Equation (\ref{generalEquil}) is tricky.

Typically, we restrict ourselves to a subset of possible Markov chains: those that satisfy detailed balance for all pairs of states $j$ and $k$:
\begin{eqnarray}
	\bm \pi_j\Pr(x_{i+1} = k| x_i = j) = \bm \pi_k\Pr(x_{i+1} = j| x_i = k) \label{theDeets}
\end{eqnarray}
Detailed balance means that for any two possible states, the flux between them balances out.

Clearly, if the flux out of $j$ into $k$ is exactly matched by the flux from $k$ into $j$, then  Equation (\ref{generalEquil}) will also be satisfied (the total flux out of $j$ will be balanced by the total flux into $j$).

Another way to express the detailed balance restriction is to put the transition probabilities on one side of the equation and the stationary frequencies on the other:
\begin{eqnarray}
	\frac{\bm \pi_j}{\bm \pi_k} = \frac{\Pr(x_{i+1} = j| x_i = k)}{\Pr(x_{i+1} = k| x_i = j)} \label{theDeetsRatio}
\end{eqnarray}


\section*{MCMC}
In the Metropolis-Hastings algorithm, we choose rules for constructing a stochastic walk through parameter space.
We adopt transition probabilities such that the stationary distribution of our Markov chain is equal to posterior probability distribution.
In notation we would like:
$$ \bm \pi_{\theta_j} = \Pr(\theta_j|\mbox{Data})$$

Let's work through an example for which we can calculated the posterior probabilities analytically, and then construct a MCMC algorithm that will converge to those posterior probabilities.

\subsection*{Double-headed coin}
Imagine a store that sells four-packs of quarters, but in some of the packs some of the coins have George Washington's head on both sides ($\Pr(H) = 1$ for these coins) while the remaining coins are fair coins ($\Pr(H)  = \Pr(T)  = 0.5$).

Further more there are equal numbers of packages that contain 0, 1, 2, 3, or 4 double-headed coins, and the packages are not labelled.

Someone grabs a package at random and records the number of heads from two experiments in which all for coins are flipped.
In the first experiment all four flips are heads.
In the second experiment three of the four flips are heads.

We want to make a probability statement about the number of coins in the pack that are double-headed.
Thus our data is $D=\{d_1=4, d_2=3\}$.
We have one parameter, $\theta$, which can take on 5 values: $\theta\in\{0,1,2,3,4\}$. 

If the five times of packs are equally likely, then we can express the prior probability of each value of $\theta$ as $1/5$.

We can calculate the probability of different number of heads, $h$, given each possible value of $\theta$ by calculating the probability of $d-\theta$ heads in $4-\theta$ flips.
We use these likelihood calculations and Bayes formula:
$$ \Pr(\theta|D) = \frac{\Pr(D|\theta)\Pr(\theta)}{\Pr(D)}= \frac{\Pr(D,\theta)}{\Pr(D)}$$ 
to produce the following table:
\begin{table}[htdp]
\begin{center}
\begin{tabular}{|c|c|c|c|c|c|c|}
\hline
& \multicolumn{5}{c|}{$\theta$} &\\
& 0 & 1 & 2 & 3 & 4 & Sum\\
\hline
$\Pr(d=4|\theta)$ & 1/16 & 2/16 & 4/16 & 8/16 & 16/16 & \\
$\Pr(d=3|\theta)$ & 4/16 & 6/16 & 8/16 & 8/16 & 0 & \\
$\Pr(d=2|\theta)$ & 6/16 & 6/16 & 4/16 & 0 & 0 & \\
$\Pr(d=1|\theta)$ & 4/16 & 2/16 & 0 & 0 & 0 & \\
$\Pr(d=0|\theta)$ & 1/16 & 0 & 0 & 0 & 0 & \\
\hline
$\sum_{i=0}^4 \Pr(d=i|\theta)$ & 1 & 1 & 1 & 1 & 1 & \\
\hline
$\Pr(d_1=4|\theta)$ & 1/16 & 2/16 & 4/16 & 8/16 & 16/16 & \\
$\Pr(d_2=3|\theta)$ & 4/16 & 6/16 & 8/16 & 8/16 & 0 & \\
$\Pr(D|\theta)$  & 4/256 & 12/256 & 32/26 & 64/256 & 0 & \\
\hline
$\Pr(\theta)$ & 0.2 & 0.2 & 0.2 & 0.2 & 0.2 & 1\\ 
\hline
$\Pr(\theta,D)$  & 4/1280 & 12/1280 & 32/1280 & 64/1280 & 0 & \\
$\Pr(D) =\sum_{\theta}\Pr(\theta,D)$ & & & & & & 112/1280 \\
\hline
$\Pr(\theta|D) = \frac{\Pr(D,\theta)}{\Pr(D)}$ & 1/28 & 3/28 & 2/7 & 4/7 & 0 & 1\\
\hline
\end{tabular}
\end{center}
\caption{Probability calculation for four coins, and data consisting of $d_1=4, d_2=3$}\label{tableCoinProbs}
\end{table}

\subsection*{MCMC}
In this case, we can simply calculate the posterior probability of $\theta$ given the data.  
But, in more complicated scenarios we cannot perform calculations for all possible values of $\theta$.
When we cannot try all possible values of $\theta$, then we can't sum them to get $\Pr(D)$.
In fact, our inability to calculate $\Pr(D)$, the marginal probability of the data, is the primary reason that we usually have to resort to MCMC to perform Bayesian inference.

Recall, that in MCMC we are going to explore the space of parameters.  
In the case of the coin-example, there are 5 possible values for $\theta$.
So we need to construct a Markov chain with 5 states.
Figure (\ref{fig5state}) shows one example.
Note that not all of the states are adjacent, in the sense that there is not an arc between all possible pairs. 
Thus, it would not be possible to move from state 0 to state 2 in a single step.
But that is fine - a Markov chain with this architecture could be constructed to sample the posterior probability distribution.
\begin{figure}[h]
\begin{tikzpicture} 
  \node[terminal] (zero) {$0$};
  \node[terminal,right=2cm of zero] (one) {$1$};
  \node[terminal,right=2cm of one] (two) {$2$};
  \node[terminal,right=2cm of two] (three) {$3$};
  \node[terminal,right=2cm of three] (four) {$4$};
  \draw[->] (zero) edge[bend left] node (f) [above]{$m_{0,1}$} (one)
  		    (one) edge[bend left] node (f) [above]{$m_{1,2}$} (two)
  		    (two) edge[bend left] node (f) [above]{$m_{2,3}$} (three)
  		    (three) edge[bend left] node (f) [above]{$m_{3,4}$} (four)
  		    (zero) edge [loop] node[above] {$m_{0,0}$} (zero)
  		    (one) edge [loop] node[above] {$m_{1,1}$} (one)
  		    (two) edge [loop] node[above] {$m_{2,2}$} (one)
  		    (three) edge [loop] node[above] {$m_{3,3}$} (one)
  		    (four) edge [loop] node[above] {$m_{4,4}$} (one)
            (four) edge[out=250, in=290, looseness=1,distance=2cm] node (h) [below]{$m_{4,0}$} (zero)
            (zero) edge[out=250, in=290, looseness=.5,distance=3cm] node (h) [below]{$m_{0,4}$} (four)
            (one) edge[bend left] node (h) [below]{$m_{1,0}$} (zero)
            (two) edge[bend left] node (h) [below]{$m_{2,1}$} (one)
            (three) edge[bend left] node (h) [below]{$m_{3,2}$} (two)
            (four) edge[bend left] node (h) [below]{$m_{4,3}$} (three)
            ;
\end{tikzpicture}
\caption{A graphical depiction of a five-state Markov process.}\label{fig5state}
\end{figure}
The $m_{j,k}$ labels on the arcs mean ``the probability of moving from $j$ to $k$.''

How can we construct a chain with the desired stationary distribution?  We have to choose transition probabilities that obey detailed balance (equation \ref{theDeetsRatio}). The other constraint is that the sum of the probabilities over all states at $i+1$ (conditional on the state at $i$) must be 1.

Specifically, we would like:
\begin{eqnarray*}
	\bm \pi_0 & = & \Pr(\theta=0|D) =  \frac{\Pr(D|\theta=0)\Pr(\theta=0)}{\Pr(D)} \\
	\bm \pi_1 & = & \Pr(\theta=1|D) =  \frac{\Pr(D|\theta=1)\Pr(\theta=1)}{\Pr(D)} \\
	\ldots
	\bm \pi_4 & = & \Pr(\theta=4|D) =  \frac{\Pr(D|\theta=4)\Pr(\theta=4)}{\Pr(D)} 
\end{eqnarray*}
Note that in our compact notation on the graph, $m_{0,1} = \Pr(x_{i+1} = 1 | x_i = 0)$, thus we can rephrase equation (\ref{theDeetsRatio}) as:
$$	\frac{\bm \pi_j}{\bm \pi_k} = \frac{m_{k,j}}{m_{j,k}} $$
So if we want to set $m_{0,1}$ and $m_{1,0}$ so that the transitions between 0 and 1 obey detailed balance and the stationary distribution is the posterior distribution we have:
\begin{eqnarray*}
	\frac{m_{1,0}}{m_{0,1}} & = & \frac{\bm \pi_0}{\bm \pi_1} \\
		 & = & \left.\left(\frac{\Pr(D|\theta=0)\Pr(\theta=0)}{\Pr(D)}\right)\right/\left(\frac{\Pr(D|\theta=1)\Pr(\theta=1)}{\Pr(D)}\right) \\
		 & = & \frac{\Pr(D|\theta=0)\Pr(\theta=0)}{\Pr(D|\theta=1)\Pr(\theta=1)}
\end{eqnarray*}
A mathematically boring, but important, thing happened in the last step: the $\Pr(D)$ crossed out.
Thus in order to express the ratio of move probabilities we don't need to be able to calculate $\Pr(D)$.

This is crucial -- in fact this is the reason we use MCMC.  
Recall that we cannot calculate $\Pr(D)$ for most ``real-world'' problems because it involves summing over states.
Fortunately, we don't have to calculate it to do MCMC.

In our coin example, we can see that $\Pr(D|\theta=1)\Pr(\theta=1)$ is three times higher than $\Pr(D|\theta=0)\Pr(\theta=0)$.
Thus we need to ensure that the probability of moving from 0$\rightarrow 1$ is 1/3 the probability of moving from $1\rightarrow0$.
For instance, we could set $m_{0,1} = 1$ and $m_{1,0} = 1/3$.
Or we could choose $m_{0,1} = 1/2$ and $m_{1,0} = 1/6$.
Which should we use?  We'd like an MCMC simulation that converges quickly so we should set the transition probabilities as high as possible.
So, using $m_{0,1} = 1$ and $m_{1,0} = 1/3$ sounds best.

Similar arguments lead us to the conclusion that $m_{1,2} = 1$ and $m_{2,1} = 3/8$.
However this reveals a problem: setting these four transition probabilities in this way means that $\Pr(x_{i+1} = 0 |x_{i} = 1) = 1/3$ and $\Pr(x_{i+1} = 2 |x_{i} = 1) = 1$.
But this violates fundamental rules of probability - the probability of all mutually-exclusive events must sum to 1.0. 
With our transition probabilities it exceeds 1.

On solution is to view a transition probability as a joint event: the move is proposed and the move is accepted.
We can denote the probability of proposing a move from $j$ to $k$ as $\prop{j}{k}$.  
We can denote the acceptance probability of a move from $j$ to $k$ (given that the move was proposed) as $\accept{j}{k}$.
If we were to use $x_{i+1}^{\prime}$ to denote the state proposed at step $i+1$ then
\begin{eqnarray*}
	\prop{j}{k} & = & \Pr(x_{i+1}^{\prime}=k|x_{i}=j) \\
	\accept{j}{k} & = & \Pr(x_{i+1}=k | x_{i}^{\prime}=j, x_{i+1}^{\prime}=k)
\end{eqnarray*}


This means that we can choose proposal probabilities that sum to one for all of the state-changing transitions.
Then, we can multiply them by the appropriate acceptance probability (keeping that as high as we can, but $\leq 1$).
So, we get
\begin{equation}\frac{m_{1,0}}{m_{0,1}} = \frac{\prop{1}{0}\accept{1}{0}}{\prop{0}{1}\accept{0}{1}} = \frac{\Pr(D|\theta=0)\Pr(\theta=0)}{\Pr(D|\theta=1)\Pr(\theta=1)} \label{moveToPropAccept}\end{equation}

We have a great deal of flexibility in selecting how we perform proposals on new states in MCMC.
We have to assure that $\prop{j}{k} > 0$ whenever $\prop{k}{j} > 0$; failing to do this means that the ratio of proposal probabilities will be 0 in one direction and infinite in the other.
It is fine for $\prop{j}{k} = \prop{k}{j} = 0$ for some pairs of states (we already said that it was OK if not all states were adjacent in the Markov chain -- note that two states that are not adjacent still fulfill detailed balance because both fluxes are 0).

Once we have chosen a proposal scheme, though, we do not have as much flexibility when choosing whether or not to accept a proposal.
Rearranging terms in equation (\ref{moveToPropAccept}) to put the acceptance probabilities on one side gives us:
\begin{eqnarray*}
	 \frac{\accept{1}{0}}{\accept{0}{1}} &  = & \frac{\Pr(D|\theta=0)\Pr(\theta=0)\prop{0}{1}}{\Pr(D|\theta=1)\Pr(\theta=1)\prop{1}{0}} \\
	 	& = & \left(\frac{\Pr(D|\theta=0)}{\Pr(D|\theta=1)}\right)\left(\frac{\Pr(\theta=0)}{\Pr(\theta=1)}\right)\left(\frac{\prop{0}{1}}{\prop{1}{0}}\right) \label{MetropHastings}
\end{eqnarray*}
or, in words,
\begin{eqnarray}
	\begin{array}{c} \mbox{acceptance} \\ \mbox{ratio} \end{array} & = & 
		\left(\begin{array}{c} {\mbox{likelihood}} \\ {\mbox{ratio}} \end{array}\right)
		\left(\begin{array}{c} {\mbox{prior}} \\ {\mbox{ratio}} \end{array}\right)
		\left(\begin{array}{c} {\mbox{Hastings}} \\ {\mbox{ratio}} \end{array}\right)
\end{eqnarray}
here the name ``Hastings ratio'' comes from \citet{Hastings1970}.
We can generalize this to any pair of adjacent states, $j$ and $k$, by substituting in $j$ for 0 and $k$ for 1:
\begin{eqnarray}
	 \frac{\accept{k}{j}}{\accept{j}{k}} & = & \left(\frac{\Pr(D|\theta=j)}{\Pr(D|\theta=k)}\right)\left(\frac{\Pr(\theta=j)}{\Pr(\theta=k)}\right)\left(\frac{\prop{j}{k}}{\prop{k}{j}}\right) \label{MetropHastings}
\end{eqnarray}
It also applies in the case of continuous parameters (with few restrictions), however then we have ratios of probability densities rather than probabilities.
The MCMC algorithm that exploits Equation (\ref{MetropHastings}) is referred to as the Metropolis-Hastings algorithm \citep{MetropolisRRTT1953}.

\subsubsection*{Metropolis-Hastings in a program}
Equation (\ref{MetropHastings}) express the fundamental constraint that we must obey when constructing an MCMC sampler. 
The last step toward being able to write a piece of software to perform this algorithm is to think about how this constraint can be implemented.
As we simulate a Markov chain, we start from a valid state.  Thus, at each step, $i$, we have a current state, $x_i$.
In an iteration:
\begin{compactenum}
	\item We propose a new state, $x_{i+1}^{\prime}$ using code that proposes move according to the probability statements encapsulated in $\prop{x_i}{j}$.
	\item We calculate an acceptance probability, $\accept{x_{i}}{x_{i+1}^{\prime}}$, for the move we proposed.
	\item If we draw a uniform random number less than this acceptance probability, then we set $x_{i+1} = x_{i+1}^{\prime}$, otherwise we set $x_{i+1} = x_{i}$.
	\item We add one to the step count $i \leftarrow i + 1$
\end{compactenum}
Note that when we reject we stay in the same state.
Thus for every ``destination'' state, $k$,  from starting state, $j$, we add $\prop{j}{k}\left(1-\accept{j}{k}\right)$ to the $j\rightarrow j$ transition probability.
We don't have to worry about enforcing detailed balance for the $j\rightarrow j$ transition (because the ``flux out'' has to be the same as the ``flux in'' for any probability we assign).

Note that we have to have an acceptance probability, while equation (\ref{MetropHastings}) gives us the ratio of probabilities between states.
The insight that we use to proceed is that we want to maximize the probability that the Markov chain move through state space.
We can calculate the acceptance ratio for any pair of states because we can calculate the likelihood, prior probability (or prior probability density), and proposal probability (or proposal probability density) at any state.

To make things slightly more concrete, let's use $A$ to represent the acceptance ratio.
Imagine that ${\accept{x_{i}}{x_{i + 1}^{\prime}}}/{\accept{x_{i}}{x_{i + 1}^{\prime}}} = A \geq 1$. 
This means that $\accept{x_{i}}{x_{i + 1}^{\prime}} > \accept{x_{i + 1}^{\prime}}{x_{i}} $.
There will be an infinite number of choices of acceptance probabilities that satisfy this, but we want to make the probabilities as big as possible.
The maximum value for a probability is 1, so we can simply set:
\begin{eqnarray*}
	\accept{x_{i}}{x_{i + 1}^{\prime}} & = &  1 \\
	\accept{x_{i + 1}^{\prime}}{x_{i}}  & = & \frac{1}{A}
\end{eqnarray*}

In the alternative scenario,  ${\accept{x_{i}}{x_{i + 1}^{\prime}}}/{\accept{x_{i}}{x_{i + 1}^{\prime}}} = A \leq 1$. We can satisfy this an maximize changing state by 
\begin{eqnarray*}
	\accept{x_{i + 1}^{\prime}}{x_{i}} & = &  1 \\
	\accept{x_{i}}{x_{i + 1}^{\prime}} & = & A
\end{eqnarray*}

As we simulate a Markov chain, we can figure what condition we are in by imagine if we had proposed the reverse move  ($x_{i + 1}^{\prime}\rightarrow x_{i}$ instead of $x_{i}\rightarrow x_{i + 1}^{\prime}$).
We can calculate the acceptance ratio for the pair of moves (forward and reverse) using equation (\ref{MetropHastings}).
If the forward probability must be higher, then we set the acceptance probability to 1.
If the forward probability must be the lower probability, then we set the acceptance probability to be equal to the acceptance ratio.

This leads to the following rule: 
\begin{eqnarray*}
	\accept{x_{i}}{x_{i + 1}^{\prime}} = \min\left[1, \left(\frac{\Pr(D|\theta=x_{i + 1}^{\prime})}{\Pr(D|\theta=x_{i})}\right)\left(\frac{\Pr(\theta=x_{i + 1}^{\prime})}{\Pr(\theta=x_{i})}\right)\left(\frac{\prop{x_{i + 1}^{\prime}}{x_{i}}}{\prop{x_{i}}{x_{i + 1}^{\prime}}}\right)\right]
\end{eqnarray*}

\subsubsection*{Double-headed coin MCMC}
Returning to the example of estimating the number of double-headed coins. We can use a proposal distribution in which we propose either of the adjacent states in the five-state Markov chain over the number of coins that are double-headed with probability 0.5.
This means that when we $\prop{j}{k}=\prop{k}{j}=0.5$ for all adjacent states.
Thus the Hastings ratio is 1 for all possible moves.

We can use the likelihoods, priors (from the table \ref{tableCoinProbs}) along with this Hastings ratio to calculate acceptance probabilities.
Multiplying acceptance probabilities by the proposal probabilities yields the Markov chain shown in Figure (\ref{fig5stateConcrete}).
Note that we do not generally calculate transition probabilities when doing MCMC - we resort to MCMC when we cannot visit all of the possible states.
\begin{figure}[h]
\begin{tikzpicture} 
  \node[terminal] (zero) {$0$};
  \node[terminal,right=2cm of zero] (one) {$1$};
  \node[terminal,right=2cm of one] (two) {$2$};
  \node[terminal,right=2cm of two] (three) {$3$};
  \node[terminal,right=2cm of three] (four) {$4$};
  \draw[->] (zero) edge[bend left] node (f) [above]{$1/2$} (one)
  		    (one) edge[bend left] node (f) [above]{$1/2$} (two)
  		    (two) edge[bend left] node (f) [above]{$1/2$} (three)
  		    (three) edge[bend left] node (f) [above]{$0$} (four)
  		    (zero) edge [loop] node[above] {$1/2$} (zero)
  		    (one) edge [loop] node[above] {$1/3$} (one)
  		    (two) edge [loop] node[above] {$5/16$} (one)
  		    (three) edge [loop] node[above] {$3/4$} (one)
  		    (four) edge [loop] node[above] {$0$} (one)
            (four) edge[out=250, in=290, looseness=1,distance=2cm] node (h) [below]{$1/2$} (zero)
            (zero) edge[out=250, in=290, looseness=.5,distance=3cm] node (h) [below]{$0$} (four)
            (one) edge[bend left] node (h) [below]{$1/6$} (zero)
            (two) edge[bend left] node (h) [below]{$3/16$} (one)
            (three) edge[bend left] node (h) [below]{$1/4$} (two)
            (four) edge[bend left] node (h) [below]{$1/2$} (three)
            ;
\end{tikzpicture}
\caption{A graphical depiction of a five-state Markov process with transition probabilities calculated for a equiprobable proposal distribution and the data-based calculations from table \ref{tableCoinProbs}. }\label{fig5stateConcrete}
\end{figure}

Figure \ref{thetaTrace} shows a trace plot of the sampled values for $\theta$.

In this example some of the transitions probabilities are 0.
This is not a problem here because these probabilities are all associated with the state $\theta=4$ which has posterior probability of 0 (because it conflicts with the data, resulting in a likelihood of 0).
If state $\theta=2$ and $\theta=4$ had {\em both} had posterior probabilities of 0, then we would not have constructed a valid (irreducible) Markov chain for the purposes of MCMC.  
There would have been no way to get from state 1 to state 3, because all of the paths would be ``blocked'' by a transition with probability 0.
This is rarely a problem, because most parameters combinations in ``real-world'' problems cannot result in posterior probabilities of 0.
If you are working with a problem in which some of the posteriors can be 0, then you have to assure that the states with non-zero posterior probability are connected in your Markov chain.

If the posteriors of some states are very low, then you can end up with very small transition probabilities.
Technically, if the transition probabilities are greater than zero, then the chain is irreducible and eventually the MCMC simulation will converge to the stationary distribution.
In practice, very low transition probabilities between different ``islands'' of states represents a very demanding inference problem.
Mixing will almost certainly be very slow, and your inference from MCMC can be misleading because any finite sampling of the Markov chain can miss very appreciable parts of parameter space.
Running independent simulations from different starting points is one way to diagnose a Markov chain that is mixing slowly.


\begin{figure}[h]
\begin{picture}(0,230)(0,-100)
	\put(-30,20){\makebox(0,0)[l]{\includegraphics[scale=0.5]{coin_contamination_out.pdf}}}
\end{picture}
\caption{Trace of $\theta$ values for the first 100 iterations of a MCMC run}\label{thetaTrace}
\end{figure}


\newpage

\appendix
\section{Code to calculate probabilities for two-state Markov chain}\label{appendixCodeTwoProb}
Python code, {\tt two\_state.py}, to generate probabilities for a certain number of iterations:
\begin{lstlisting}[frame=single,language=Python,columns=fixed,upquote=false,showstringspaces=false]
#!/usr/bin/env python
import sys
from random import random as uniform

a_to_b = float(sys.argv[1])
assert(a_to_b > 0.0 and a_to_b <= 1.0)
b_to_a = float(sys.argv[2])
assert(b_to_a > 0.0 and b_to_a <= 1.0)
ti_prob_list = [a_to_b, b_to_a]
b_to_b = 1 - b_to_a
state = int(sys.argv[4])
assert(state in [0,1])
if state == 0:
    prob_list = [1.0, 0.0]
else:
    prob_list = [0.0, 1.0]
num_it = int(sys.argv[3])
assert(num_it > 0)

print "Gen\tPrZero\tPrOne"
for i in xrange(num_it):
    print "\t".join([str(i)] + [str(p) for p in prob_list])
    pb = prob_list[0]*a_to_b + prob_list[1]*b_to_b
    prob_list = [1-pb, pb]
\end{lstlisting}
R code, {\tt plotProbTrace.R}, to create plots from a file with columns labelled ``Gen'' and ``PrZero'':
\begin{lstlisting}[frame=single,language=S,columns=fixed,upquote=false,showstringspaces=false]
fn = commandArgs(TRUE)
d = read.table(fn, header=TRUE, sep="\t");
pdf(paste(fn, '.pdf', sep=""));
plot(d$Gen, d$PrZero, xlab="i", ylab="Pr(x_i=0)", type="l", ylim=c(0,1));
dev.off();
\end{lstlisting}
bash invocation using the scripts to create pdf documents:
\begin{lstlisting}[frame=single,language=bash,columns=fixed,upquote=false,showstringspaces=false]
$ python two_state.py 0.6 0.9 20 0 >start_from_0.txt
$ R --no-save -f plotProbTrace.R --args start_from_0.txt
$ python two_state.py 0.6 0.9 20 1 >start_from_1.txt
$ R --no-save -f plotProbTrace.R --args start_from_1.txt
\end{lstlisting}

\newpage
\section{Snippet of code for double-headed coin MCMC}

\begin{lstlisting}[frame=single,language=Python,columns=fixed,upquote=false,showstringspaces=false]
likelihood = calc_likelihood(state)
assert(likelihood > 0.0)
counts = [0]*(num_coins + 1)
sys.stderr.write("Gen\tlike\ttheta\n")
for i in xrange(num_it):
    sys.stderr.write("%d\t%f\t%d\n" % (i, likelihood, state))
    counts[state] += 1
    prev_likelihood = likelihood
    if random.random() < 0.5:
        proposed = state + 1
        if state > num_coins:
            proposed = 0
    else:
        proposed = state - 1
        if state < 0:
            proposed = num_coins
	# Prior ratio is 1.0, se we can ignore it
	
	# Hastings ratio is 1.0, se we can ignore it
	
    likelihood = calc_likelihood(proposed)
    if likelihood > prev_likelihood:
        state = proposed
    else:
        if random.random() < likelihood/prev_likelihood:
            state = proposed
        else:
            likelihood = prev_likelihood
            
print "Posterior probabilities:"
for state in range(num_coins + 1):
    print state, float(counts[state])/num_it
\end{lstlisting}
Full program at \url{http://phylo.bio.ku.edu/slides/coin_contamination.py.txt}

\newpage
\bibliography{phylo}
\end{document}  


\subsection*{GLM in Bayesian world}
Consider a data set of different diets many full sibs reared under two diets (normal=0, and unlimited=1).
We measure snout-vent length for a bunch of gekkos.
Our model is:
\begin{compactitem}
	\item There is an unknown mean SVL under the normal diet, $\alpha_0$.
	\item $\alpha_1$ is the mean SVL of an infinitely-large independent sample under the unlimited diet.
	\item Each family, $j$, will have a mean effect, $B_j$. This effect gets added to the mean based on the diet, regardless of diet.
	\item Each family, $j$, will have a mean response to unlimited diet, $C_{1j}$. This effect is only added to individuals on the unlimited diet. For notational convenience, we can simply define $C_{0j}=0$ for all families.
	\item The SVL for each individual is expected to normally-distributed around the expected value; the difference between a response and the expected value is $\epsilon_{ijk}$.
\end{compactitem}
To complete the likelihood model, we have to say something about the probability distributions that govern the random effects:
\begin{compactitem}
	\item $B_j \sim \mathcal{N}(0, \sigma_B)$
	\item $C_{1j} \sim \mathcal{N}(0, \sigma_C)$
	\item $\epsilon_{ijk} \sim \mathcal{N}(0, \sigma_E)$
\end{compactitem}

In our previous approach, we could do a hypothesis test such as $H_0: \alpha_0 = \alpha_1$, or we could generate point estimates.
That is OK, but what if we want to answer questions such as ``What is the probability that $\alpha_1 - \alpha_0 > 0.5$mm?''

Could we:
\begin{compactitem}
	\item reparameterize to $\delta_1 = \alpha_1 - \alpha_1$,
	\item construct a $x\%$ confidence interval,
	\item search for the largest value of $x^{\dag}$ such that 0 is not included in the confidence interval?
	\item do something like $\Pr(\alpha_1 - \alpha_0 > 0.5) = (1-x^{\dag})/2$
\end{compactitem}
, or something like that? {\bf No!} That is not a correct interpretation of a $P$-value, or confidence interval!


